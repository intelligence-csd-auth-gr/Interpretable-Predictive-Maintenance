{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LioNets: Turbofan Engine Degradation Simulation Dataset with Neural Networks -> Classification Task\n",
    "\n",
    "In this notebook, we present how LioNets can be applied in predictive models using time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG\n",
    "from IPython.display import display                               \n",
    "from ipywidgets import interactive, BoundedFloatText, FloatSlider, IntSlider, ToggleButtons,  \\\n",
    "    RadioButtons, IntRangeSlider, Dropdown, jslink, jsdlink, interactive_output, HBox, VBox, Label\n",
    "from load_dataset import Load_Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from math import sqrt, exp, log\n",
    "from sklearn.linear_model import Lasso, Ridge, RidgeCV, SGDRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score, balanced_accuracy_score, accuracy_score\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, TimeDistributed, RepeatVector,Flatten, \\\n",
    "    Input, Dropout, LSTM, concatenate, Reshape, Conv1D, GlobalMaxPool1D\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "from LioNets import LioNet\n",
    "from nbeats_keras.model import NBeatsNet\n",
    "from Interpretable_PCA import iPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we load and clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, feature_names = Load_Dataset.load_data_turbofan(False)\n",
    "\n",
    "fm1_train = fm['FaultMode1']['df_train']\n",
    "fm1_train_target = fm1_train['RUL'].values\n",
    "fm1_test= fm['FaultMode1']['df_test']\n",
    "fm1_test_target = fm1_test['RUL'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dropping some unecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_train = fm1_train.drop(columns=['t', 'os_1', 'os_2', 'os_3', 's_01', 's_05', 's_06', 's_10', 's_16', 's_18', 's_19', 's_22', 's_23', 's_24', 's_25', 's_26'])\n",
    "LSTM_test = fm1_test.drop(columns=['t', 'os_1', 'os_2', 'os_3', 's_01', 's_05', 's_06', 's_10', 's_16', 's_18', 's_19', 's_22', 's_23', 's_24', 's_25', 's_26'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We collect the different units, in order to the next steps to create time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_units = set(LSTM_train['u'].values)\n",
    "test_units = set(LSTM_test['u'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are scaling our data per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = ['s_02', 's_03', 's_04', 's_07', 's_08', 's_09', 's_11', 's_12',\n",
    "            's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "scalers = {}\n",
    "for column in sensors:\n",
    "    scaler = MinMaxScaler(feature_range=(0.1,1.1))\n",
    "    LSTM_train[column] = scaler.fit_transform(LSTM_train[column].values.reshape(-1,1))\n",
    "    LSTM_test[column] = scaler.transform(LSTM_test[column].values.reshape(-1,1))\n",
    "    scalers[column] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create time windows with a specific size. In this example, we create time windows of 50 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_scalers = {}\n",
    "window = 50\n",
    "temp_LSTM_x_train = []\n",
    "LSTM_y_train = []\n",
    "for unit in train_units:\n",
    "    temp_unit = LSTM_train[LSTM_train['u']==unit].drop(columns=['u','RUL']).values\n",
    "    temp_unit_RUL = LSTM_train[LSTM_train['u']==unit]['RUL'].values\n",
    "    \n",
    "    for i in range(len(temp_unit) - window + 1):#elekse edw an len temp_unit - window > 0\n",
    "        temp_instance = []\n",
    "        for j in range(window):\n",
    "            temp_instance.append(temp_unit[i+j])\n",
    "        temp_LSTM_x_train.append(np.array(temp_instance))\n",
    "        LSTM_y_train.append(temp_unit_RUL[i+window-1])\n",
    "LSTM_y_train = np.array(LSTM_y_train)\n",
    "LSTM_x_train = np.array(temp_LSTM_x_train)\n",
    "\n",
    "temp_LSTM_x_test = []\n",
    "LSTM_y_test = []\n",
    "for unit in test_units:\n",
    "    temp_unit = LSTM_test[LSTM_test['u']==unit].drop(columns=['u','RUL']).values\n",
    "    temp_unit_RUL = LSTM_test[LSTM_test['u']==unit]['RUL'].values\n",
    "        \n",
    "    for i in range(len(temp_unit) - window + 1):#elekse edw an len temp_unit - window > 0\n",
    "        temp_instance = []\n",
    "        for j in range(window):\n",
    "            temp_instance.append(temp_unit[i+j])\n",
    "        temp_LSTM_x_test.append(np.array(temp_instance))\n",
    "        LSTM_y_test.append(temp_unit_RUL[i+window-1])\n",
    "LSTM_y_test = np.array(LSTM_y_test)\n",
    "LSTM_x_test = np.array(temp_LSTM_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many train, test instances we have. These are changing regarding the time window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_x_train.shape, LSTM_x_test.shape, LSTM_y_train.shape, LSTM_y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to transform our RUL to binary classes. 0 Would mean that no maintenance is needed, because the prediction had a high RUL value. 1 would mean that the RUL is low and you may need maintenance on your component! You can try different time frames as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_frame = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_LSTM_y_train = np.array([1 if i <= time_frame else 0 for i in LSTM_y_train])\n",
    "temp_LSTM_y_test = np.array([1 if i <= time_frame else 0 for i in LSTM_y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need a rmse loss function too! for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build our predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = fm1_train.columns\n",
    "encoder_input = Input(shape=(LSTM_x_train[0].shape))\n",
    "\n",
    "encoder_x = LSTM(units=80, return_sequences=True, activation='tanh')(encoder_input)\n",
    "encoder_x = Dropout(0.5)(encoder_x)\n",
    "encoder_x = LSTM(units=40, return_sequences=False, activation='tanh')(encoder_x)\n",
    "\n",
    "encoder_y = Conv1D(filters=40,kernel_size=3,activation='tanh')(encoder_input)\n",
    "encoder_y = GlobalMaxPool1D()(encoder_y)\n",
    "\n",
    "encoded = concatenate([encoder_x,encoder_y])\n",
    "encoded = Dropout(0.5)(encoded)\n",
    "encoded = Dense(80, activation='tanh')(encoded)#Relu and selu\n",
    "encoded = Dropout(0.5)(encoded)\n",
    "encoded = Dense(40, activation='tanh')(encoded)#Relu and selu\n",
    "predictions = Dense(1, activation='sigmoid')(encoded)#Relu and selu\n",
    "predictor = Model(encoder_input,predictions)\n",
    "\n",
    "predictor.compile(optimizer=\"adam\",loss=['binary_crossentropy'],metrics=['accuracy'])\n",
    "#print(predictor.summary())\n",
    "\n",
    "checkpoint_name = 'TEDS_Predictor_Classification.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predictor.fit(LSTM_x_train, temp_LSTM_y_train, epochs=250, batch_size=512, shuffle=True, validation_split=0.33, verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our weights, and we measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'TEDS_Predictor_Classification.hdf5' # choose the best checkpoint few features\n",
    "predictor.load_weights(wights_file) # load it\n",
    "predictor.compile(optimizer=\"adam\",loss=[root_mean_squared_error],metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pred = predictor.predict(LSTM_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pred = predictor.predict(LSTM_x_train)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_pred]\n",
    "print('Train:',f1_score(temp_LSTM_y_train,predictions, average='micro'),f1_score(temp_LSTM_y_train,predictions, average='weighted'),balanced_accuracy_score(temp_LSTM_y_train,predictions),accuracy_score(temp_LSTM_y_train,predictions))\n",
    "\n",
    "temp_pred = predictor.predict(LSTM_x_test)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_pred]\n",
    "print('Test:',f1_score(temp_LSTM_y_test,predictions, average='micro'),f1_score(temp_LSTM_y_test,predictions, average='weighted'),balanced_accuracy_score(temp_LSTM_y_test,predictions),accuracy_score(temp_LSTM_y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to extract the encoder from our predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input=predictor.input, output=[predictor.layers[-2].output])\n",
    "encoder.trainable = False\n",
    "encoder.compile(optimizer=\"adam\",loss=[root_mean_squared_error],metrics=['mae','mse'])\n",
    "#encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract for all instances, their encoded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_LSTM_x_train = encoder.predict(LSTM_x_train)\n",
    "encoded_LSTM_x_test = encoder.predict(LSTM_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by that, we build the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(encoded_LSTM_x_train[0].shape))\n",
    "decoded = Dense(120, activation='tanh')(encoded_input)\n",
    "decoded = Dropout(0.5)(decoded)\n",
    "\n",
    "decoded_y = RepeatVector(54)(decoded)\n",
    "decoded_y = Conv1D(filters=50,kernel_size=5,activation='tanh')(decoded_y)\n",
    "\n",
    "decoded_x = RepeatVector(50)(decoded)\n",
    "decoded_x = LSTM(units=80, return_sequences=True, activation='tanh')(decoded_x)\n",
    "decoded_x = Dropout(0.5)(decoded_x)\n",
    "decoded_x = LSTM(units=50, return_sequences=True, activation='tanh')(decoded_x)\n",
    "\n",
    "decoded = concatenate([decoded_x,decoded_y])\n",
    "decoded = Dense(50, activation='sigmoid')(decoded)\n",
    "decoded = Dropout(0.5)(decoded)\n",
    "decoded = Dense(14, activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = Model(encoded_input,decoded)\n",
    "\n",
    "decoder.compile(optimizer=\"adam\",loss=[root_mean_squared_error],metrics=['mae','mse'])\n",
    "#print(decoder.summary())\n",
    "\n",
    "checkpoint_name = 'TEDS_Decoder_Classification.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder.fit(encoded_LSTM_x_train, LSTM_x_train, epochs=250, batch_size=512, shuffle=True, validation_split=0.33, verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'TEDS_Decoder_Classification.hdf5' # choose the best checkpoint few features\n",
    "decoder.load_weights(wights_file) # load it\n",
    "decoder.compile(optimizer=\"adam\",loss=[root_mean_squared_error],metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_LSTM_x_train,LSTM_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_LSTM_x_test,LSTM_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "forecast_timesteps = 5\n",
    "\n",
    "temp_fc_x_train = []\n",
    "temp_fc_y_train = []\n",
    "for unit in train_units:\n",
    "    temp_unit = LSTM_train[LSTM_train['u']==unit].drop(columns=['u','RUL']).values\n",
    "   \n",
    "    for i in range(len(temp_unit) - window - forecast_timesteps + 1):#elekse edw an len temp_unit - window > 0\n",
    "        temp_instance_x = []\n",
    "        temp_instance_y = []\n",
    "        for j in range(window):\n",
    "            temp_instance_x.append(temp_unit[i+j])\n",
    "        for z in range(forecast_timesteps):\n",
    "            temp_instance_y.append(temp_unit[i+j+z+1])            \n",
    "        temp_fc_x_train.append(np.array(temp_instance_x))\n",
    "        temp_fc_y_train.append(np.array(temp_instance_y))       \n",
    "fc_x_train = np.array(temp_fc_x_train)\n",
    "fc_y_train = np.array(temp_fc_y_train)\n",
    "\n",
    "temp_fc_x_test = []\n",
    "temp_fc_y_test = []\n",
    "for unit in test_units:\n",
    "    temp_unit = LSTM_test[LSTM_test['u']==unit].drop(columns=['u','RUL']).values\n",
    "        \n",
    "    for i in range(len(temp_unit) - window - forecast_timesteps + 1):#elekse edw an len temp_unit - window > 0\n",
    "        temp_instance_x = []\n",
    "        temp_instance_y = []\n",
    "        for j in range(window):\n",
    "            temp_instance_x.append(temp_unit[i+j])\n",
    "        for z in range(forecast_timesteps):\n",
    "            temp_instance_y.append(temp_unit[i+j+z+1])            \n",
    "        temp_fc_x_test.append(np.array(temp_instance_x))\n",
    "        temp_fc_y_test.append(np.array(temp_instance_y))       \n",
    "fc_x_test = np.array(temp_fc_x_test)\n",
    "fc_y_test = np.array(temp_fc_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_x_train.shape, fc_x_test.shape, fc_y_train.shape, fc_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_input = Input(shape=(LSTM_x_train[0].shape))\n",
    "\n",
    "forecast_x = LSTM(units=120, return_sequences=True, activation='tanh')(forecast_input)\n",
    "forecast_x = Dropout(0.7)(forecast_x)\n",
    "forecast_x = LSTM(units=50, return_sequences=True, activation='tanh')(forecast_x)\n",
    "forecast_x = Conv1D(filters=50,kernel_size=46,activation='tanh')(forecast_x)\n",
    "\n",
    "forecast_y = Conv1D(filters=50,kernel_size=46,activation='tanh')(forecast_input)\n",
    "\n",
    "forecast = concatenate([forecast_y,forecast_x])\n",
    "forecast = Dropout(0.7)(forecast)\n",
    "forecast = LSTM(40, return_sequences=True, activation='relu')(forecast)#Relu and selu\n",
    "forecast = Dropout(0.7)(forecast)\n",
    "predictions = LSTM(14, return_sequences=True, activation='linear')(forecast)#Relu and selu\n",
    "forecaster = Model(forecast_input,predictions)\n",
    "forecaster.summary()\n",
    "forecaster.compile(optimizer=\"adam\", loss=[root_mean_squared_error],metrics=['mae','mse'])\n",
    "\n",
    "checkpoint_name = 'TEDS_Forecaster_Classification_Matrix.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#forecaster.fit(fc_x_train, fc_y_train, epochs=250, batch_size=512, shuffle=True, validation_split=0.3, verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'TEDS_Forecaster_Classification_Matrix.hdf5' # choose the best checkpoint few features\n",
    "forecaster.load_weights(weights_file) # load it\n",
    "forecaster.compile(optimizer=\"adam\",loss=[root_mean_squared_error],metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = forecaster.predict(fc_x_train)\n",
    "# print('Train:',mean_absolute_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)),mean_squared_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)),sqrt(mean_squared_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70))))\n",
    "# print(r2_score(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = forecaster.predict(fc_x_test)\n",
    "# print('Test:',mean_absolute_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)),mean_squared_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)),sqrt(mean_squared_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70))))\n",
    "# print(r2_score(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## N-Beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definition of the model.\n",
    "nbeats = NBeatsNet(input_dim = 14, backcast_length=50, forecast_length=5,\n",
    "                  stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK), \n",
    "                  nb_blocks_per_stack=2,\n",
    "                  thetas_dim=(4, 4), \n",
    "                  share_weights_in_stack=True, \n",
    "                  hidden_layer_units=64)\n",
    "\n",
    "# Definition of the objective function and the optimizer.\n",
    "nbeats.compile_model(loss='mae', learning_rate=1e-6)\n",
    " \n",
    "# Train the model.\n",
    "#nbeats.fit(fc_x_train, fc_y_train, verbose=1, validation_split=0.3, epochs=100, batch_size=128)\n",
    "\n",
    "# Save the model for later.\n",
    "#nbeats.save('n_beats_model.h5')\n",
    "\n",
    "# # Load the model.\n",
    "nbeats = NBeatsNet.load('n_beats_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = nbeats.predict(fc_x_train)\n",
    "# print('Train:',mean_absolute_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)),mean_squared_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)),sqrt(mean_squared_error(fc_y_train.reshape(-1,70),predictions.reshape(-1,70))))\n",
    "# print(r2_score(fc_y_train.reshape(-1,70),predictions.reshape(-1,70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = nbeats.predict(fc_x_test)\n",
    "# print('Test:',mean_absolute_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)),mean_squared_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)),sqrt(mean_squared_error(fc_y_test.reshape(-1,70),predictions.reshape(-1,70))))\n",
    "# print(r2_score(fc_y_test.reshape(-1,70),predictions.reshape(-1,70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sens = 10\n",
    "j = 123\n",
    "temp_instance = fc_x_train[j].copy()[np.newaxis]\n",
    "nbts = nbeats.predict(temp_instance)\n",
    "pred = forecaster.predict(temp_instance)\n",
    "ln = temp_instance.shape[1]\n",
    "plt.plot(np.arange(ln), temp_instance.squeeze()[:,sens], color='black')\n",
    "plt.plot(np.arange(ln-1,ln+5), np.append(temp_instance.squeeze()[-1:,sens],fc_y_test[j][:,sens]), color='g') # Ground Truth\n",
    "plt.plot(np.arange(ln-1,ln+5), np.append(temp_instance.squeeze()[-1:,sens],pred.squeeze()[:,sens]), color='b') # Neural Forecast \n",
    "plt.plot(np.arange(ln-1,ln+5), np.append(temp_instance.squeeze()[-1:,sens],nbts.squeeze()[:,sens]), color='r') # NBeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## XYZ7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "forecast_steps = 5\n",
    "\n",
    "rul_train, xyz7_x_train, xyz7_y_train, rul_temp = [],[],[],[]\n",
    "for unit in train_units:\n",
    "    temp_unit = LSTM_train[LSTM_train['u']==unit].drop(columns=['u','RUL']).values   \n",
    "    for i in range(len(temp_unit) - window + 1): # elekse edw an len temp_unit - window > 0\n",
    "        temp_instance = np.array(temp_unit[i:i+window])\n",
    "        rul_temp.append(temp_instance)\n",
    "        xyz7_x_train.append(temp_instance[:-forecast_steps])\n",
    "        xyz7_y_train.append(temp_instance[-forecast_steps:])\n",
    "\n",
    "rul_train = predictor.predict(np.array(rul_temp))\n",
    "xyz7_x_train = np.array(xyz7_x_train)\n",
    "xyz7_y_train = np.array(xyz7_y_train)\n",
    "\n",
    "rul_test, xyz7_x_test, xyz7_y_test, rul_temp = [],[],[],[]\n",
    "for unit in test_units:\n",
    "    temp_unit = LSTM_test[LSTM_test['u']==unit].drop(columns=['u','RUL']).values   \n",
    "    for i in range(len(temp_unit) - window + 1): # elekse edw an len temp_unit - window > 0\n",
    "        temp_instance = np.array(temp_unit[i:i+window])\n",
    "        rul_temp.append(temp_instance)\n",
    "        xyz7_x_test.append(temp_instance[:-forecast_steps])\n",
    "        xyz7_y_test.append(temp_instance[-forecast_steps:])\n",
    "\n",
    "rul_test = predictor.predict(np.array(rul_temp))\n",
    "xyz7_x_test = np.array(xyz7_x_test)\n",
    "xyz7_y_test = np.array(xyz7_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_train.shape , xyz7_x_train.shape , xyz7_y_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forecast_input = Input(shape=(xyz7_x_train[0].shape))\n",
    "rul_input = Input(shape = (rul_train[0].shape))\n",
    "\n",
    "forecast_x = LSTM(units=120, return_sequences=True, activation='tanh')(forecast_input)\n",
    "forecast_x = Dropout(0.7)(forecast_x)\n",
    "forecast_x = LSTM(units=50, return_sequences=True, activation='tanh')(forecast_x)\n",
    "forecast_x = Conv1D(filters=50,kernel_size=41,activation='tanh')(forecast_x)\n",
    "\n",
    "forecast_y = Conv1D(filters=50,kernel_size=41,activation='tanh')(forecast_input)\n",
    "\n",
    "rul = RepeatVector(5)(rul_input)\n",
    "forecast = concatenate([forecast_y, forecast_x, rul])\n",
    "forecast = Dropout(0.7)(forecast)\n",
    "forecast = LSTM(40, return_sequences=True, activation='relu')(forecast)#Relu and selu\n",
    "forecast = Dropout(0.7)(forecast)\n",
    "predictions = LSTM(14, return_sequences=True, activation='linear')(forecast)#Relu and selu\n",
    "\n",
    "xyz7_model = Model([forecast_input, rul_input],predictions)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "xyz7_model.compile(optimizer=opt, loss=[root_mean_squared_error],metrics=['mae','mse'])\n",
    "\n",
    "checkpoint_name = 'TEDS_XYZ7_Classification.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xyz7_model.fit([xyz7_x_train,rul_train], xyz7_y_train, epochs=250, batch_size=512, shuffle=True, validation_split=0.3, verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## LioNets & Interpretable PCA Experiments \n",
    "Having everything setted up, we are now ready to try our methodology. We first initialize LioNets. LioNets requires a predictor (the classifier itself), an encoder (extracted from the predictor), a decoder, as well as some data (for best results the training data, in order to push the neighbourhood generation through known distribution for the network). \n",
    "\n",
    "We also test the intrepretable capabilities of PCA. Using the LioNet from above we generate a neighbourhood around a test instance and then apply per-sensor PCA reducing the sensor readings dimensionality from 50 (timesteps) to 1. Then we fit the transformed data with their predictions to a Rigde Regression model to acquire the weights(importance) of each sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lionet = LioNet(predictor, decoder, encoder, LSTM_x_train)\n",
    "ipca = iPCA(lionet.give_me_the_neighbourhood, 2000, 'local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would like to manually evaluate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_instance = LSTM_x_train[112].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LioNets weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.0001,fit_intercept=True,random_state=0)\n",
    "lionet_weights, real_prediction, local_prediction = lionet.explain_instance(temp_instance,200,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretable PCA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sensor_weights, pca_timestep_weights = ipca.find_importance(temp_instance)\n",
    "pca_timestep_weights = pca_timestep_weights.reshape(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {'LioNets':lionet_weights,'Interpretable PCA':pca_timestep_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data statistics -> (Global Mean & STD per sensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = LSTM_x_train.reshape(-1,14)\n",
    "global_mean, global_std = [],[]\n",
    "for i in range(14):\n",
    "    global_mean.append(temp_train[:,i].mean())\n",
    "    global_std.append(temp_train[:,i].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make modifications to the measurements of a seleced sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(temp_instance, weights, sens, mod, uni_sldr=0, rd_btn=1, rng_sldr=(1,50)):\n",
    "    \n",
    "    start, end = rng_sldr[0], rng_sldr[1]\n",
    "    \n",
    "    mod_instance = temp_instance.copy()\n",
    "    local_mean = temp_instance[start-1:end,sens].mean()\n",
    "      \n",
    "    # ---MODS---        \n",
    "    if mod == 1: # Uniform\n",
    "        for i in range(start-1, end):\n",
    "            if weights.reshape(50,14)[i,sens] > 0 and rd_btn > 0:\n",
    "                mod_instance[i,sens] = mod_instance[i,sens] + uni_sldr\n",
    "            if weights.reshape(50,14)[i,sens] < 0 and rd_btn < 0:\n",
    "                mod_instance[i,sens] = mod_instance[i,sens] + uni_sldr    \n",
    "    elif mod == 2: # Local MEan\n",
    "        mod_instance[start-1:end, sens] = local_mean    \n",
    "    elif mod == 3: # Global Mean \n",
    "        mod_instance[start-1:end, sens] = global_mean[sens]   \n",
    "    elif mod == 4: # Zeros\n",
    "        mod_instance[start-1:end, sens] = 0.1    \n",
    "    elif mod == 5: # Gaussian Noise\n",
    "        for i in range(start-1, end):\n",
    "            np.random.seed(2000+i)\n",
    "            gaussian_noise = np.random.normal(global_mean[sens], global_std[sens], 1)/10\n",
    "            mod_instance[i,sens] += gaussian_noise[0]\n",
    "        np.clip(mod_instance,0.1,1.1,out=mod_instance)    \n",
    "    elif mod == 6: # Neural Forecaster\n",
    "        prediction = forecaster.predict(np.expand_dims(temp_instance,axis=0))\n",
    "        prediction = prediction.squeeze()\n",
    "        mod_instance = np.append(temp_instance,prediction,axis=0)\n",
    "        mod_instance = mod_instance[5:]\n",
    "    elif mod == 7: # Static Forecaster\n",
    "        for i in range(mod_instance.shape[1]):\n",
    "            dif = mod_instance[-1,i] - mod_instance[-6:-1,i]\n",
    "            temp = np.flip(dif) + mod_instance[-1,i]\n",
    "            #temp = np.array([dif*(e+1) for e,i in enumerate(range(5))]) + mod_instance[-1,i]\n",
    "            mod_instance[:,i] = np.append(mod_instance[5:,i],temp)  \n",
    "            np.clip(mod_instance[:,i],0.1,1.1,out=mod_instance[:,i])\n",
    "    elif mod == 8: # NBeats Forecaster\n",
    "        prediction = nbeats.predict(np.expand_dims(temp_instance,axis=0))\n",
    "        prediction = prediction.squeeze()\n",
    "        mod_instance = np.append(temp_instance,prediction,axis=0)\n",
    "        mod_instance = mod_instance[5:]\n",
    "        \n",
    "    return mod_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moded_instance_statistics(instance, interpret_method):\n",
    "    \n",
    "    model = Ridge(alpha=0.0001,fit_intercept=True,random_state=0)\n",
    "    weights, real_prediction, local_prediction = lionet.explain_instance(instance,200,model)\n",
    "    weights = weights * instance.reshape(700)\n",
    "    if interpret_method == 'Interpretable PCA':\n",
    "        _, weights = ipca.find_importance(instance)\n",
    "        weights = weights.reshape(700)\n",
    "    \n",
    "    sensors_all = {}\n",
    "    count = 0\n",
    "    for j in range(50):\n",
    "        count2 = 0\n",
    "        for i in sensors:\n",
    "            sensors_all.setdefault(i,[]).append([j, weights[count+count2], instance[j][count2],\n",
    "                                                 weights[count+count2]*instance[j][count2]])\n",
    "            count2 = count2 + 1\n",
    "        count = count + 14\n",
    "        \n",
    "    sensors_std = []\n",
    "    sensors_mean = []\n",
    "    sensors_max = []\n",
    "    sensors_min = []\n",
    "    for i in sensors_all:\n",
    "        naa = np.array(sensors_all[i])[:,3]\n",
    "        sensors_std.append(naa.std())\n",
    "        sensors_mean.append(naa.mean())\n",
    "        sensors_max.append(naa.max())\n",
    "        sensors_min.append(naa.min())\n",
    "        \n",
    "    return [real_prediction, local_prediction], sensors_all, [sensors_mean,sensors_std,sensors_min,sensors_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 2 sensors with the most negative and positive influence based on the mean of time-step feature weights.\n",
    "\n",
    "Then recommend certain modifications that can be applied on their measurements that lead to a change in the RUL propability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_modifications(instance, weights, interpret_method):\n",
    "    \n",
    "    _, _, original_sens_stats = moded_instance_statistics(instance,interpret_method)\n",
    "    sensors_mean =  original_sens_stats[0]\n",
    "    indexed = list(enumerate(sensors_mean))\n",
    "    indexed.sort(key=lambda tup: tup[1])\n",
    "    cls0_sens = list([i for i, v in indexed[:2]])\n",
    "    cls1_sens = list(reversed([i for i, v in indexed[-2:]]))\n",
    "#     print(\"Class 0 important sensors:\",sensors[cls0_sens[0]], sensors[cls0_sens[1]])\n",
    "#     print(\"Class 1 important sensors:\",sensors[cls1_sens[0]], sensors[cls1_sens[1]])\n",
    "\n",
    "    mods = ['Original', 'Uniform', 'Mean(Local)', 'Mean(Global)', 'Zero', \\\n",
    "            'Noise', 'Forecast (Neural)', 'Forecast (Static)', 'Forecast (N-Beats)']\n",
    "    wghts = ['Negative Weights', 'Positive Weights']\n",
    "    \n",
    "    cls0_mod_results = []\n",
    "    cls1_mod_results = []\n",
    "    unif_tests= [0.1, 0.5, -0.1, -0.5]\n",
    "    \n",
    "    for sens in cls0_sens:\n",
    "        temp = []\n",
    "        for v,w in zip(unif_tests,np.sign(unif_tests)):\n",
    "            mod_inst = modify(instance, weights, sens, 1, v, w)\n",
    "            mod_preds = predictor.predict(np.array([mod_inst,mod_inst]))[0]\n",
    "            temp.append((mod_preds[0],sens,1,v,w))#it was mod_preds[1]\n",
    "        for mod in range(2,len(mods)):\n",
    "            mod_inst = modify(instance, weights, sens, mod)\n",
    "            mod_preds = predictor.predict(np.array([mod_inst,mod_inst]))[0]\n",
    "            temp.append((mod_preds[0],sens,mod))#it was mod_preds[1]\n",
    "        cls0_mod_results.append(max(temp))\n",
    "\n",
    "    for sens in cls1_sens:\n",
    "        temp = []\n",
    "        for v,w in zip(unif_tests,-np.sign(unif_tests)):\n",
    "            mod_inst = modify(instance, weights, sens, 1, v, w)\n",
    "            mod_preds = predictor.predict(np.array([mod_inst,mod_inst]))[0]\n",
    "            temp.append((mod_preds[0],sens,1,v,w))#it was mod_preds[1]\n",
    "        for mod in range(2,len(mods)):\n",
    "            mod_inst = modify(instance, weights, sens, mod)\n",
    "            mod_preds = predictor.predict(np.array([mod_inst,mod_inst]))[0]\n",
    "            temp.append((mod_preds[0],sens,mod))#it was mod_preds[1]\n",
    "        cls1_mod_results.append(min(temp))\n",
    "\n",
    "\n",
    "    recommendation = \"\\t\\t\\t\\t\\t\\t<<< Recommendations >>>\\n\\n\"\n",
    "    for e0,rec in enumerate(cls0_mod_results):\n",
    "        if rec[2]==1:\n",
    "            recommendation += str(e0+1)+\") Try the Uniform modification on sensor \"+str(sensors[rec[1]])+\\\n",
    "            \" with Value: \"+str(rec[3])+\" on the \"+str(wghts[int((1+rec[4])/2)])+\" to increase the RUL propability.\\n\"\n",
    "        else:\n",
    "            recommendation += str(e0+1)+\") Try the \"+str(mods[rec[2]])+\" modification on sensor \"+str(sensors[rec[1]])+ \\\n",
    "            \" to increase the RUL propability.\\n\"\n",
    "       \n",
    "    for e1,rec in enumerate(cls1_mod_results):\n",
    "        if rec[2]==1:\n",
    "            recommendation += str(e1+e0+2)+\") Try the Uniform modification on sensor \"+str(sensors[rec[1]])+\\\n",
    "            \" with Value: \"+str(rec[3])+\" on the \"+str(wghts[int((1+rec[4])/2)])+\" to decrease the RUL propability.\\n\"\n",
    "        else:\n",
    "            recommendation += str(e1+e0+2)+\") Try the \"+str(mods[rec[2]])+\" modification on sensor \"+str(sensors[rec[1]])+ \\\n",
    "            \" to decrease the RUL propability.\\n\"\n",
    "            \n",
    "    return recommendation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the new modified instances and inspect the changes of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original stats\n",
    "original_preds, original_sens_all, original_sens_stats = {},{},{}\n",
    "for method in weights_dict.keys():\n",
    "    original_preds[method], original_sens_all[method], original_sens_stats[method] = \\\n",
    "    moded_instance_statistics(temp_instance,method)\n",
    "\n",
    "# Recommend modifications\n",
    "recommendation = {}\n",
    "for method in weights_dict.keys():\n",
    "    recommendation[method] = recommend_modifications(temp_instance, weights_dict[method], method)  # Lionets & IPCA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeSens =  1\n",
    "def plot_sensor(sens_i, mod_sens_i, mod, rng_sldr, uni_sldr, rd_btn, interpret_method):\n",
    "    \n",
    "    global seeSens, mod_preds, mod_sens_all, mod_sens_stats\n",
    "    \n",
    "    # Recommend modifications\n",
    "    print(recommendation[interpret_method])\n",
    "    \n",
    "    # Disable/Enable UI elements\n",
    "    uniform_slider.disabled, radio_button.disabled = True, True\n",
    "    modify_sens_i.disabled, range_slider.disabled =  False, False\n",
    "    if mod==1:\n",
    "        uniform_slider.disabled, radio_button.disabled = False, False\n",
    "    if mod==0 or mod==6 or mod==7 or mod==8:\n",
    "        modify_sens_i.disabled, range_slider.disabled = True, True\n",
    "    \n",
    "    # If a UI element has been changed other than the Sensor View proceed to the modification\n",
    "    if seeSens == sens_i:\n",
    "        inst_mod = modify(temp_instance, weights_dict[interpret_method], mod_sens_i-1, mod, uni_sldr, rd_btn, rng_sldr)\n",
    "        mod_preds, mod_sens_all, mod_sens_stats = moded_instance_statistics(inst_mod, interpret_method)\n",
    "    else:\n",
    "        seeSens = sens_i\n",
    "        \n",
    "    # Print the predictions of RUL for the original and modified instance \n",
    "    print(\"ORIGINAL -> Real prediction: \" + str(original_preds[interpret_method][0])[:7] + \\\n",
    "                   \", Local prediction: \" + str(original_preds[interpret_method][1])[:7])\n",
    "    print(\"  MOD    -> Real prediction: \" + str(mod_preds[0])[:7] + \", Local prediction: \" + str(mod_preds[1])[:7])\n",
    "    \n",
    "    # Plotting the figures \n",
    "    to_vis = [i[2:] for i in sensors]\n",
    "    x = np.arange(len(to_vis))\n",
    "    width = 0.4\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4), dpi=200)\n",
    "    axs[0].bar(x-width, original_sens_stats[interpret_method][0], width=width, tick_label=to_vis, align='edge', color='C0')\n",
    "    axs[0].bar(x, mod_sens_stats[0], width=width, tick_label=to_vis, align='edge', color='C1')\n",
    "    axs[0].set_title('Mean')\n",
    "    axs[0].legend(('ÎŸriginal','Modded'))\n",
    "    axs[1].bar(x-width, original_sens_stats[interpret_method][1], width=width, tick_label=to_vis, align='edge', color='C0')\n",
    "    axs[1].bar(x, mod_sens_stats[1], width=width, tick_label=to_vis, align='edge', color='C1')\n",
    "    axs[1].set_title('STD')\n",
    "    axs[2].bar(x-width, original_sens_stats[interpret_method][2], width=width, tick_label=to_vis, align='edge', color='C0',)\n",
    "    axs[2].bar(x-width, original_sens_stats[interpret_method][3], width=width, tick_label=to_vis, align='edge', color='C0')\n",
    "    axs[2].bar(x, mod_sens_stats[2], width=width, tick_label=to_vis, align='edge', color='C1')\n",
    "    axs[2].bar(x, mod_sens_stats[3], width=width, tick_label=to_vis, align='edge', color='C1')\n",
    "    axs[2].set_title('Max and Min')  \n",
    "    #for i in rec_sens:\n",
    "    #    org_means[i].set_ecolor('r')\n",
    "    \n",
    "    #fig.suptitle('Sensor Importance Statistics')\n",
    "    plt.show()\n",
    "\n",
    "    TIMESTEPS = np.arange(temp_instance.shape[0])\n",
    "    \n",
    "    plt.figure(figsize=(14, 4), dpi=200, facecolor='w', edgecolor='k')\n",
    "    plt.subplot(131)\n",
    "    plt.plot(TIMESTEPS,np.array(original_sens_all[interpret_method][sensors[sens_i-1]])[:,1],color='grey',linestyle = ':')\n",
    "    plt.plot(TIMESTEPS,np.array(mod_sens_all[sensors[sens_i-1]])[:,1],color='tab:blue')\n",
    "    plt.hlines(y=np.array(mod_sens_all[sensors[sens_i-1]])[:,1].mean(), xmin=0, xmax=50, label='mean')\n",
    "    plt.title(str(\"Sensor\\'s \" + sensors[sens_i-1] + \" influence\"))\n",
    "    plt.subplot(132)\n",
    "    plt.plot(TIMESTEPS,np.array(original_sens_all[interpret_method][sensors[sens_i-1]])[:,2],color='grey',linestyle = ':')\n",
    "    plt.plot(TIMESTEPS,np.array(mod_sens_all[sensors[sens_i-1]])[:,2],color='g')\n",
    "    plt.hlines(y=np.array(mod_sens_all[sensors[sens_i-1]])[:,2].mean(), xmin=0, xmax=50, label='mean')\n",
    "    plt.title(str(\"Sensor\\'s \" + sensors[sens_i-1] + \" value\"))\n",
    "    plt.subplot(133)\n",
    "    plt.plot(TIMESTEPS,np.array(original_sens_all[interpret_method][sensors[sens_i-1]])[:,3],color='grey',linestyle = ':')\n",
    "    plt.plot(TIMESTEPS,np.array(mod_sens_all[sensors[sens_i-1]])[:,3],color='r')\n",
    "    plt.hlines(y=np.array(mod_sens_all[sensors[sens_i-1]])[:,3].mean(), xmin=0, xmax=50, label='mean')\n",
    "    plt.title(str(\"Sensor\\'s \" + sensors[sens_i-1] + \" influence * value\"))\n",
    "    plt.show()\n",
    "    \n",
    "                            ### Setting up the interactive visualization tool ###\n",
    "\n",
    "# UI elements\n",
    "range_slider = IntRangeSlider(value=[1,50], min=1, max=50, description=\"Range: \", continuous_update = False)\n",
    "view_sens_i = IntSlider(min=1, max=14, default_value=2, description=\"View Sensor: \", continuous_update = False)\n",
    "modify_sens_i = IntSlider(min=1, max=14, default_value=2, description=\"Mod Sensor: \", continuous_update = False)\n",
    "uniform_slider = FloatSlider(value=0, min=-1.1, max=1.1, step=0.05, description='Value:', continuous_update = False)\n",
    "radio_button = RadioButtons(options=[('Positive Weights', 1), ('Negative Weights', -1)], description='Affect:')\n",
    "interpret_method = ToggleButtons(options=['LioNets', 'Interpretable PCA'])\n",
    "mod = Dropdown(options=[('Original', 0), ('Uniform', 1), ('Mean (Local)', 2), ('Mean (Global)', 3), ('Zero', 4), ('Noise', 5),\n",
    "                        ('Forecast (Neural)', 6), ('Forecast (Static)', 7), ('Forecast (N-Beats)', 8)], description=\"Mods: \")\n",
    "jsdlink((modify_sens_i, 'value'), (view_sens_i, 'value'))\n",
    "\n",
    "# UI layout\n",
    "interpretable_settings = HBox([Label('Interpretation method:'), interpret_method])\n",
    "interpretable_settings.layout.margin = '0 0 20px 0'\n",
    "mod_settings = HBox([VBox([modify_sens_i,view_sens_i]), VBox([mod, range_slider]), VBox([uniform_slider, radio_button])])\n",
    "ui = VBox([interpretable_settings, mod_settings])\n",
    "\n",
    "# Starting the interactive tool\n",
    "inter = interactive_output(plot_sensor, {'sens_i':view_sens_i, 'mod_sens_i':modify_sens_i, \n",
    "                                         'mod':mod, 'rng_sldr':range_slider, 'uni_sldr':uniform_slider, \n",
    "                                         'rd_btn':radio_button, 'interpret_method': interpret_method})\n",
    "display(ui,inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
